# What Enables In-Context Learning in Transformers?
### An Analysis of Attention Mechanisms

This website presents our DSC 180B capstone project in a public-facing format.

---

## Overview
- High-level summary of the project
- What in-context learning (ICL) is
- Why attention mechanisms might matter

---

## Introduction
- Motivation: Why study in-context learning?
- Why transformers are relevant
- Our central research question

---

## Background
### What is In-Context Learning (ICL)?
- Intuitive explanation
- Why it is surprising / powerful

### What is Attention?
- High-level explanation of attention in transformers
- Why different attention variants exist

---

## Research Question
- Does the structure of the attention mechanism affect a transformer's ability to perform in-context learning?
- What trade-offs exist between efficiency and learning behavior?

---

## Methods
- Description of experimental setup
- Attention variants compared:
  - Standard multi-head attention
  - Grouped Query Attention (GQA)
  - Sparse attention variant
  - Gated attention variant
- Evaluation metrics (e.g., prediction error vs. number of in-context examples)

---

## Results
- Comparison of ICL performance across attention types
- Analysis of performance in different regimes (e.g., underparameterized vs. overparameterized)
- Identification of trade-offs and failure modes

---

## Key Takeaways
- Summary of main findings
- What we learned about attention and ICL

---

## Impact and Implications
- Why this matters for model design
- Efficiency vs. capability trade-offs
- Potential applications or future work

---

## Team
- **Anish Kasam** — [akasam@ucsd.edu](mailto:akasam@ucsd.edu)
- **Dhanvi Patel** — [dhp003@ucsd.edu](mailto:dhp003@ucsd.edu)
- **Krish Prasad** — [krprasad@ucsd.edu](mailto:krprasad@ucsd.edu)
- **Shou Tai Yue** — [syue@ucsd.edu](mailto:syue@ucsd.edu)

---

## Links
- Project report PDF (to be added)
- [Code repository](https://github.com/Shou-Yue/Incontext-Learning-of-Attention-Mechanisms/tree/main)
- Poster or presentation (to be added)